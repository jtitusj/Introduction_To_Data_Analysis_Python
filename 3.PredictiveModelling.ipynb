{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Modelling\n",
    "The process of building and optimizing models based from given set of variables/features to predict a target or several target variables.  \n",
    "\n",
    "There several approaches of doing this task. To name a few, we have **statistical modelling**, **machine learning**, and **deep learning**. \n",
    "\n",
    "Most of us here are familiar with Statistical Modelling (regression models, time-series models, survival models, ...). In many cases, statistical models have assumptions such as linear independence of features/predictor variables and that the data must be taken from an assumed distribution. \n",
    "\n",
    "With Machine Learning, we don't necessarily rely on underlying assumptions from statistics. Most of the techniques used in this approach utilizes concepts from Mathematics such as multivariate calculus and linear algebra such as *gradients* and *hessian matrices* and Computer Science concepts like *greedy algorithms*.\n",
    "\n",
    "Lastly, with Deep Learning, existing models try to mimic the human brain. In the early years of this branch of research, Deep Learning models were just theoretical since there were no powerful computers during that time. However, with the advancement of technology and the depreciation of the cost of more powerful computers, Deep Learning is reemerging and is actually becoming more and more successful in its goals.\n",
    "\n",
    "In our workshop, we will discuss how we can perform Predictive Modelling using Python. Specifically, we will use models related to Statistical Modelling and Machine Learning only since both learning about the concept and the computations requires more time.\n",
    "\n",
    "In Python, the most widely used package for Predictive Modelling is **Scikit-Learn**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is scikit-learn?\n",
    "[Scikit-learn](http://scikit-learn.org/stable/) is an open-source machine learning module. The scikit-learn project is constantly being developed and improved, and it has a very active user community. The documentation on the website is very thorough with plenty of examples, and there are a number of tutorials and guides for learning how scikit-learn works.\n",
    "\n",
    "![Scikit-Learn](images/scikit-learn-logo.png)\n",
    "\n",
    "### Why scikit-learn?\n",
    "Scikit-learn is open source, it's free to use! Also, it's currently the most comprehensive machine learning tool for Python. There are also a number of Python libraries that work well with scikit-learn and extend its capabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some terms\n",
    "\n",
    "- **observation/instance/data point**: these all mean the same thing, and that is one particular piece of the data that we can grab information about and learn relationships from.\n",
    "- **label/class**: in classification, the label/class is what we aim to classify our new data as. Ex: email as spam or not spam.\n",
    "- **feature**: features describe the data. Features of email spam could be number of capital letter or frequency of known spam words.\n",
    "- **categorical**: discrete and finite data; has categories. Ex. spam or not spam.\n",
    "- **continuous**: subset of real numbers, can take on any value between two points. Ex. temperature degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of machine learning\n",
    "\n",
    "#### Supervised\n",
    "Supervised learning is machine learning that makes use of labeled data. Supervised learning algorithms can use past observations to make future predictions on both categorical and continuous data. The two main types of supervised learning are classification and regression. Classification predicts labels, such as spam or not spam. Regression predicts the relationship between continuous variables, such as the relationship between temperature and elevation.\n",
    "\n",
    "#### Unsupervised\n",
    "Unsupervised learning is used when the data is unlabeled. You might not know what you're looking for within your data, and unsupervised learning can help you figure it out. Clustering is an example of unsupervised learning, where data instances are grouped together in a way that observations in the same group are more similar to each other than to those in other groups. Another example is dimensionality reduction, where the number of random variables is reduced, and is used for both feature selection and feature extraction.\n",
    "\n",
    "![](images/ml_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Let's start with classification\n",
    "Classification, again, classifies data into specific categories, and solves the task of figuring out which category new data belong to. There are many different kinds of classifiers, and which one you want to use depends on your data. We're only going to be covering k-Nearest Neighbors (kNN), the Naive Bayes classifier (NB), Decision Trees because they're among the simplest to implement and understand.\n",
    "\n",
    "For both algorithms, I'll walk you through simple examples of each, so that you'll have an idea of how they work. I'll also show you how to evaluate the models we create.\n",
    "\n",
    "Something important to notice in my examples is that when we train, we use a different dataset than when we predict. This is to avoid the problem of overfitting. So, what's overfitting? Well, let's say we train our model on the entire dataset. If we want to also test with that dataset, we won't be able to get an accurate picture of how good our model is, because now it knows our entire dataset by heart. This is why we split up our sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbors\n",
    "*\"Tell me who your friends are and I tell you who you are.\"*\n",
    "\n",
    "The k-Nearest Neighbors (kNN) algorithm finds a predetermined number of \"neighbor\" samples that are closest in distance to a starting data point and makes predictions based on the distances. kNN predicts labels by looking at the labels of its nearest neighbors. The metric used to calculate the distances between points can be any distance metric measure. The similarity measure is dependent on the type of data. For real-valued data, the [Euclidean distance](https://reference.wolfram.com/language/ref/EuclideanDistance.html) can be used. Other other types of data such as categorical or binary data, [Hamming distance](http://classroom.synonym.com/calculate-hamming-distance-2656.html) can be used.\n",
    "\n",
    "![KNNClassification](images/KnnClassification.png)\n",
    "\n",
    "kNN is useful when your data is linear in nature and can therefore be measured with a distance metric. Also, kNN does well when the decision boundary (or the delineation between classes) is hard to identify. \n",
    "\n",
    "kNN comes with a couple of caveats. If the classes in your dataset are unevenly distributed, the highest-occuring label will tend to dominate predictions. Also, choosing the *k* of kNN can be tricky. Choosing *k* deserves its own three hour tutorial, so we'll just go with the defaults for today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying in scikit-learn: kNN\n",
    "\n",
    "As we go through these examples, you'll notice that the basic fitting and predicting process is basically the same, which is one of the things that makes scikit-learn relatively easy to use.\n",
    "\n",
    "Let's start by reading in some data and its labels, and then split it up so we don't overfit. The default split for the `train_test_split()` function is 0.25, meaning that 75% of the data is split into the training set and 25% is split into the test set. If you want a different split, that's something that can be changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the Wine Data Set from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Wine) which are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars (a plant variety that has been produced in cultivation by selective breeding). The analysis determined the quantities of 13 constituents found in each of the three types of wines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/User/Documents/JupyterNotebooks/Introduction_To_Data_Analysis_Python'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "wine_data = pd.read_csv('data/wine_data.csv')\n",
    "wine_labels = pd.read_csv('data/wine_labels.csv', squeeze=True)\n",
    "\n",
    "wine_data_train, wine_data_test, wine_labels_train, wine_labels_test = train_test_split(wine_data, wine_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn can actually understand `DataFrames`, and can use them as input. Here's what one row from `wine_data_train` looks like, which is a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abv</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyacins</th>\n",
       "      <th>color</th>\n",
       "      <th>hue</th>\n",
       "      <th>dilution</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>12.29</td>\n",
       "      <td>2.83</td>\n",
       "      <td>2.22</td>\n",
       "      <td>18.0</td>\n",
       "      <td>88</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.15</td>\n",
       "      <td>1.15</td>\n",
       "      <td>3.30</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>13.82</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.42</td>\n",
       "      <td>14.0</td>\n",
       "      <td>111</td>\n",
       "      <td>3.88</td>\n",
       "      <td>3.74</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.87</td>\n",
       "      <td>7.05</td>\n",
       "      <td>1.01</td>\n",
       "      <td>3.26</td>\n",
       "      <td>1190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>12.08</td>\n",
       "      <td>1.83</td>\n",
       "      <td>2.32</td>\n",
       "      <td>18.5</td>\n",
       "      <td>81</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1.08</td>\n",
       "      <td>2.27</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>14.21</td>\n",
       "      <td>4.04</td>\n",
       "      <td>2.44</td>\n",
       "      <td>18.9</td>\n",
       "      <td>111</td>\n",
       "      <td>2.85</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.25</td>\n",
       "      <td>5.24</td>\n",
       "      <td>0.87</td>\n",
       "      <td>3.33</td>\n",
       "      <td>1080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>13.50</td>\n",
       "      <td>1.81</td>\n",
       "      <td>2.61</td>\n",
       "      <td>20.0</td>\n",
       "      <td>96</td>\n",
       "      <td>2.53</td>\n",
       "      <td>2.61</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.66</td>\n",
       "      <td>3.52</td>\n",
       "      <td>1.12</td>\n",
       "      <td>3.82</td>\n",
       "      <td>845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      abv  malic_acid   ash  alcalinity  magnesium  total_phenols  flavanoids  \\\n",
       "93  12.29        2.83  2.22        18.0         88           2.45        2.25   \n",
       "52  13.82        1.75  2.42        14.0        111           3.88        3.74   \n",
       "90  12.08        1.83  2.32        18.5         81           1.60        1.50   \n",
       "45  14.21        4.04  2.44        18.9        111           2.85        2.65   \n",
       "24  13.50        1.81  2.61        20.0         96           2.53        2.61   \n",
       "\n",
       "    nonflavanoid_phenols  proanthocyacins  color   hue  dilution  proline  \n",
       "93                  0.25             1.99   2.15  1.15      3.30      290  \n",
       "52                  0.32             1.87   7.05  1.01      3.26     1190  \n",
       "90                  0.52             1.64   2.40  1.08      2.27      480  \n",
       "45                  0.30             1.25   5.24  0.87      3.33     1080  \n",
       "24                  0.28             1.66   3.52  1.12      3.82      845  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the lengths of the original `DataFrame` and the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178 133\n"
     ]
    }
   ],
   "source": [
    "print(len(wine_data), len(wine_data_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit kNN to our training data. This is pretty easy. We create our estimator object and then use the fit() function to fit the algorithm to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(wine_data_train, wine_labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's use our fitted model to predict on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, 2, 3, 1, 2, 1, 2, 2, 2, 1, 1, 2, 2, 3, 1, 1, 1, 3, 1, 1, 2,\n",
       "       2, 2, 1, 2, 2, 1, 1, 2, 2, 1, 2, 2, 2, 2, 3, 2, 2, 1, 2, 3, 2, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict(wine_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>true_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    predicted  true_val\n",
       "0           2         2\n",
       "1           1         1\n",
       "2           2         2\n",
       "3           2         2\n",
       "4           3         3\n",
       "5           1         2\n",
       "6           2         2\n",
       "7           1         1\n",
       "8           2         2\n",
       "9           2         2\n",
       "10          2         2\n",
       "11          1         1\n",
       "12          1         1\n",
       "13          2         2\n",
       "14          2         2\n",
       "15          3         3\n",
       "16          1         3\n",
       "17          1         1\n",
       "18          1         1\n",
       "19          3         3\n",
       "20          1         1\n",
       "21          1         1\n",
       "22          2         2\n",
       "23          2         3\n",
       "24          2         3\n",
       "25          1         1\n",
       "26          2         2\n",
       "27          2         2\n",
       "28          1         1\n",
       "29          1         2\n",
       "30          2         2\n",
       "31          2         2\n",
       "32          1         1\n",
       "33          2         3\n",
       "34          2         2\n",
       "35          2         3\n",
       "36          2         3\n",
       "37          3         3\n",
       "38          2         2\n",
       "39          2         3\n",
       "40          1         1\n",
       "41          2         3\n",
       "42          3         2\n",
       "43          2         3\n",
       "44          1         1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'true_val':wine_labels_test.values, 'predicted':knn.predict(wine_data_test)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there are some differences between the predictions and the actual labels. Let's actually calculate how accurate our classifier is. We can do that using cross-validation. Cross-validation is a method that takes a dataset, randomly splits it into training and test sets, and computes how accurate the model is by checking it against the real labels. It does this multiple times, and splits the dataset differently each time. \n",
    "\n",
    "The `cross_val_score()` function takes several parameters. The first is the model you've fitted (in this case it's knn), the second is the entire dataset, the second is the entire list of labels, and if you'd like you can specify how many times you want to cross-validate (the cv parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.7027027 ,  0.66666667,  0.63888889,  0.65714286,  0.76470588])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "knn_cross_val = cross_val_score(knn, wine_data, wine_labels, cv=5)\n",
    "knn_cross_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68602139955081121"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average cross val score\n",
    "np.mean(knn_cross_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our model is approximately 70% accurate. That's not so great, but you get the idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson: classification with kNN!\n",
    "\n",
    "The next data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We're going to be using scikit-learn's built in datasets for this.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "iris_data = iris.data\n",
    "iris_labels = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data into training and test sets?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now, let's use the training data and labels to train our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And now, let's predict on our test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's compare the predictions to the actual labels. Output the real labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's score our model using cross-validation to see how good it is.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees\n",
    "\n",
    "Decision trees are predictive models that learn simple decision rules based on the features within a dataset. They map observations about an item to conclusions about the item's target value. Leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees in machine learning are made of the same decision trees that are used in game theory or decision analysis. \n",
    "\n",
    "Decision trees are great for heterogeneous data, having the ability to handle both categorical and continuous data; however, they are fairly simple in nature and can lack the ability to capture rich relationships within a dataset.\n",
    "\n",
    "![Titanic_Survival](images/CART_tree_titanic_survivors.png)\n",
    "\n",
    "### Classifying in scikit-learn: decision trees\n",
    "\n",
    "We're going to basically do the same thing we just did, but with a different classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, 2, 1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 3, 3, 1, 1, 3, 1, 1, 2,\n",
       "       3, 3, 1, 2, 2, 1, 2, 1, 2, 1, 3, 2, 3, 3, 3, 2, 3, 1, 3, 2, 3, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(wine_data_train, wine_labels_train)\n",
    "tree.predict(wine_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>true_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     predicted  true_val\n",
       "111          2         2\n",
       "30           1         1\n",
       "115          2         2\n",
       "122          2         2\n",
       "150          1         3\n",
       "78           2         2\n",
       "103          2         2\n",
       "2            1         1\n",
       "83           1         2\n",
       "76           2         2\n",
       "99           2         2\n",
       "54           1         1\n",
       "8            1         1\n",
       "92           2         2\n",
       "127          2         2\n",
       "137          3         3\n",
       "141          3         3\n",
       "55           1         1\n",
       "49           1         1\n",
       "131          3         3\n",
       "21           1         1\n",
       "50           1         1\n",
       "123          2         2\n",
       "162          3         3\n",
       "146          3         3\n",
       "15           1         1\n",
       "89           2         2\n",
       "79           2         2\n",
       "56           1         1\n",
       "69           2         2\n",
       "126          1         2\n",
       "75           2         2\n",
       "23           1         1\n",
       "135          3         3\n",
       "85           2         2\n",
       "161          3         3\n",
       "136          3         3\n",
       "134          3         3\n",
       "63           2         2\n",
       "152          3         3\n",
       "31           1         1\n",
       "159          3         3\n",
       "59           2         2\n",
       "147          3         3\n",
       "51           1         1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'true_val':wine_labels_test, 'predicted':tree.predict(wine_data_test)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's cross-validate again. Let's only run it four times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.82222222,  0.86666667,  0.91111111,  0.86046512])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_dtree = cross_val_score(tree, wine_data, wine_labels, cv=4)\n",
    "cross_val_dtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86511627906976751"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_dtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is a much better fit for our dataset, and is much more accurate than k-nearest neighbors.\n",
    "\n",
    "## Lesson: classification with decision trees!\n",
    "\n",
    "This dataset is made up of 1797 8x8 images. Each image, like the one shown below, is of a hand-written digit. In order to utilize an 8x8 figure like this, we’d have to first transform it into a feature vector with length 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We're going to be using scikit-learn's built in datasets for this.\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "digits_data = digits.data\n",
    "digits_labels = digits.target\n",
    "\n",
    "# Once again, split the data into training and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x116f28a20>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVgAAAFdCAYAAABGoXXzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkpTk/wB0wpgeoy6T5AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1144b09b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(digits_data[0].reshape((8,8)), cmap=plt.cm.gray_r, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1176a1fd0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVgAAAFdCAYAAABGoXXzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfypgfqErf1d9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116fdbcf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(digits_data[1].reshape((8,8)), cmap=plt.cm.gray_r, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11802f208>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVgAAAFdCAYAAABGoXXzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1176bd908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(digits_data[2].reshape((8,8)), cmap=plt.cm.gray_r, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit the model to our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict on the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Look at the test set labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finally, cross-validate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "The Naive Bayes classifier is a probabilistic classifier based on Bayes' Theorem, which states that the probability of *A* given the probability of *B* is equal to the probability of *B* given *A* times the probability of *A*, divided by the probability of *B*. In Naive Bayes classification, the classifier assumes that the features in your dataset are independent of each other; that is, one feature being a certain way has no effect on what values the other features take. This is a naive assumption because this doesn't always hold true in reality, but despite this naivety and oversimplified assumptions, the classifier performs decently and even quite well in certain classification situations.\n",
    "\n",
    "The Naive Bayes classifier is useful **when your features are independent** and your **data is normally distributed**. More sophisticated methods generally perform better.\n",
    "\n",
    "![Bayes_rule](images/Bayes_rule.png)\n",
    "\n",
    "\n",
    "### Classifying in scikit-learn: Naive Bayes\n",
    "\n",
    "Just as we did the first two times, let's do it again. We're going to use the GaussianNB estimator object, because our data is for the most part normally distributed. We're also going to use the same wine training and test sets we made earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, 2, 3, 2, 2, 1, 3, 2, 2, 1, 1, 2, 2, 3, 3, 1, 1, 3, 1, 1, 2,\n",
       "       3, 3, 1, 2, 2, 1, 2, 2, 2, 1, 3, 2, 3, 3, 3, 2, 3, 1, 3, 2, 3, 1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(wine_data_train, wine_labels_train)\n",
    "gnb.predict(wine_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that was easy! Let's look at the real test labels again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>true_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     predicted  true_val\n",
       "111          2         2\n",
       "30           1         1\n",
       "115          2         2\n",
       "122          2         2\n",
       "150          3         3\n",
       "78           2         2\n",
       "103          2         2\n",
       "2            1         1\n",
       "83           3         2\n",
       "76           2         2\n",
       "99           2         2\n",
       "54           1         1\n",
       "8            1         1\n",
       "92           2         2\n",
       "127          2         2\n",
       "137          3         3\n",
       "141          3         3\n",
       "55           1         1\n",
       "49           1         1\n",
       "131          3         3\n",
       "21           1         1\n",
       "50           1         1\n",
       "123          2         2\n",
       "162          3         3\n",
       "146          3         3\n",
       "15           1         1\n",
       "89           2         2\n",
       "79           2         2\n",
       "56           1         1\n",
       "69           2         2\n",
       "126          2         2\n",
       "75           2         2\n",
       "23           1         1\n",
       "135          3         3\n",
       "85           2         2\n",
       "161          3         3\n",
       "136          3         3\n",
       "134          3         3\n",
       "63           2         2\n",
       "152          3         3\n",
       "31           1         1\n",
       "159          3         3\n",
       "59           2         2\n",
       "147          3         3\n",
       "51           1         1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'true_val':wine_labels_test, 'predicted':gnb.predict(wine_data_test)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course, let's cross-validate to see how well we did. Let's only run it four times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.91111111,  0.95555556,  0.97777778,  1.        ])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_nb = cross_val_score(gnb, wine_data, wine_labels, cv=4)\n",
    "cross_val_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96111111111111114"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! This classifier does much better on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson: classification with Naive Bayes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We're going to be using scikit-learn's built in datasets for this.\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "digits_data = digits.data\n",
    "digits_labels = digits.target\n",
    "\n",
    "# Once again, split the data into training and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit the model to our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Look at the test set labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finally, cross-validate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n",
    "Linear regression is used when the target value is expected to be a linear combination of the input variables. The goal of linear regression, in creating a linear model, is to minimize the sum of squared residuals between the observed data and the responses predicted by linear approximation. Linear regression can be used to represent the relationship between variables like temperature and elevation, or something like housing prices and square footage.\n",
    "\n",
    "![regression](images/ex2regression.png)\n",
    "\n",
    "Linear regression is appropriate when your data is continuous and linear.\n",
    "\n",
    "### Linear regression in scikit-learn\n",
    "\n",
    "Let's try this on subset of our wine data, since those values are continuous other than `wine_type`. Let's see what the relationship is between magnesium and abv. First, let's subset the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>magnesium</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>127</td>\n",
       "      <td>5.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>4.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>5.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>113</td>\n",
       "      <td>7.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>118</td>\n",
       "      <td>4.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   magnesium  color\n",
       "0        127   5.64\n",
       "1        100   4.38\n",
       "2        101   5.68\n",
       "3        113   7.80\n",
       "4        118   4.32"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_data_mag = wine_data.loc[:, ['magnesium', 'color']]\n",
    "wine_data_abv = wine_data.loc[:, 'abv']\n",
    "wine_data_mag.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, as always, let's split up the data. Our target values are going to be the continuous abv values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wine_mag_train, wine_mag_test, wine_abv_train, wine_abv_test = train_test_split(wine_data_mag, wine_data_abv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we fit the model to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(wine_mag_train, wine_abv_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 13.16137391,  12.65332416,  12.43415507,  13.02731024,\n",
       "        12.47153349,  12.91896641,  13.03135243,  12.33005172,\n",
       "        12.94111572,  12.76085955,  12.45507678,  12.8143054 ,\n",
       "        13.2987836 ,  12.64623921,  13.15258606,  12.67689571,\n",
       "        13.03099304,  12.98789579,  13.32257602,  13.39144926,\n",
       "        13.23479087,  13.07729781,  12.13501185,  13.43638792,\n",
       "        13.40344462,  13.91564882,  12.19942008,  13.06097961,\n",
       "        12.54958384,  12.88755761,  13.48841153,  13.67209974,\n",
       "        12.51756124,  12.93249627,  13.70387157,  13.19303348,\n",
       "        12.92895197,  12.98789579,  13.32801786,  12.60498336,\n",
       "        12.37473961,  12.72883695,  13.34525676,  12.92663892,  12.67536121])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict(wine_mag_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare those predictions to the actual abv values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>true_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>13.161374</td>\n",
       "      <td>13.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>12.653324</td>\n",
       "      <td>12.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>12.434155</td>\n",
       "      <td>12.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>13.027310</td>\n",
       "      <td>12.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>12.471533</td>\n",
       "      <td>12.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>12.918966</td>\n",
       "      <td>13.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>13.031352</td>\n",
       "      <td>12.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>12.330052</td>\n",
       "      <td>11.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>12.941116</td>\n",
       "      <td>12.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>12.760860</td>\n",
       "      <td>13.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>12.455077</td>\n",
       "      <td>12.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>12.814305</td>\n",
       "      <td>13.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>13.298784</td>\n",
       "      <td>12.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>12.646239</td>\n",
       "      <td>12.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13.152586</td>\n",
       "      <td>14.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>12.676896</td>\n",
       "      <td>13.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>13.030993</td>\n",
       "      <td>13.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>12.987896</td>\n",
       "      <td>13.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.322576</td>\n",
       "      <td>14.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13.391449</td>\n",
       "      <td>14.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>13.234791</td>\n",
       "      <td>14.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>13.077298</td>\n",
       "      <td>12.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>12.135012</td>\n",
       "      <td>12.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>13.436388</td>\n",
       "      <td>13.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>13.403445</td>\n",
       "      <td>13.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>13.915649</td>\n",
       "      <td>13.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>12.199420</td>\n",
       "      <td>12.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>13.060980</td>\n",
       "      <td>13.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>12.549584</td>\n",
       "      <td>11.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12.887558</td>\n",
       "      <td>14.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>13.488412</td>\n",
       "      <td>13.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>13.672100</td>\n",
       "      <td>13.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>12.517561</td>\n",
       "      <td>12.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.932496</td>\n",
       "      <td>14.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>13.703872</td>\n",
       "      <td>13.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>13.193033</td>\n",
       "      <td>13.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>12.928952</td>\n",
       "      <td>13.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12.987896</td>\n",
       "      <td>13.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>13.328018</td>\n",
       "      <td>12.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>12.604983</td>\n",
       "      <td>12.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>12.374740</td>\n",
       "      <td>12.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>12.728837</td>\n",
       "      <td>12.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>13.345257</td>\n",
       "      <td>12.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>12.926639</td>\n",
       "      <td>12.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>12.675361</td>\n",
       "      <td>12.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     predicted  true_val\n",
       "165  13.161374     13.73\n",
       "70   12.653324     12.29\n",
       "86   12.434155     12.16\n",
       "133  13.027310     12.70\n",
       "122  12.471533     12.42\n",
       "34   12.918966     13.51\n",
       "69   13.031352     12.21\n",
       "103  12.330052     11.82\n",
       "137  12.941116     12.53\n",
       "41   12.760860     13.41\n",
       "92   12.455077     12.69\n",
       "145  12.814305     13.16\n",
       "135  13.298784     12.60\n",
       "100  12.646239     12.08\n",
       "10   13.152586     14.10\n",
       "24   12.676896     13.50\n",
       "51   13.030993     13.83\n",
       "141  12.987896     13.36\n",
       "0    13.322576     14.23\n",
       "5    13.391449     14.20\n",
       "39   13.234791     14.22\n",
       "163  13.077298     12.96\n",
       "89   12.135012     12.08\n",
       "52   13.436388     13.82\n",
       "174  13.403445     13.40\n",
       "176  13.915649     13.17\n",
       "119  12.199420     12.00\n",
       "42   13.060980     13.88\n",
       "127  12.549584     11.79\n",
       "29   12.887558     14.02\n",
       "152  13.488412     13.11\n",
       "168  13.672100     13.58\n",
       "118  12.517561     12.77\n",
       "11   12.932496     14.12\n",
       "169  13.703872     13.40\n",
       "161  13.193033     13.69\n",
       "25   12.928952     13.05\n",
       "12   12.987896     13.75\n",
       "147  13.328018     12.87\n",
       "91   12.604983     12.00\n",
       "129  12.374740     12.04\n",
       "67   12.728837     12.37\n",
       "160  13.345257     12.36\n",
       "134  12.926639     12.51\n",
       "60   12.675361     12.33"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'true_val':wine_abv_test, 'predicted':lr.predict(wine_mag_test)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the accuracy of our linear regression model by using the `score()` function. The `score()` function returns the R^2 coefficient, which is a measure of how far away from the actual values are predictions were. The closer to 1, the better the regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34946751283608846"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(wine_mag_test, wine_abv_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our score is rather low.\n",
    "\n",
    "## Lesson: linear regression!\n",
    "\n",
    "The dataset  contains information about the housing values in suburbs of Boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We're going to be using scikit-learn's built in datasets for this.\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "boston_data = boston.data\n",
    "boston_target = boston.target\n",
    "\n",
    "# Once again, split the data into training and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new linear regression model and fit the model to our data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict on the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take a look at the actual target values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Score the model!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "Scikit-learn:\n",
    "\n",
    "https://github.com/sarguido/hands-on-analysis-python\n",
    "\n",
    "https://canvas.northwestern.edu/courses/20122/assignments/syllabus\n",
    "\n",
    "https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\n",
    "\n",
    "https://en.wikipedia.org/wiki/Decision_tree_learning\n",
    "\n",
    "http://www.saedsayad.com/naive_bayesian.htm\n",
    "\n",
    "http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&doc=exercises/ex2/ex2.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
